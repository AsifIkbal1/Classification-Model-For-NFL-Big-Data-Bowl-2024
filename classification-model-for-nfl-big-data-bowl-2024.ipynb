{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":60305,"databundleVersionId":6654553,"sourceType":"competition"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-08T14:45:55.260813Z","iopub.execute_input":"2024-01-08T14:45:55.261481Z","iopub.status.idle":"2024-01-08T14:45:55.270054Z","shell.execute_reply.started":"2024-01-08T14:45:55.261448Z","shell.execute_reply":"2024-01-08T14:45:55.269084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since 2017, the National Football League has collaborated with Amazon Web Services to collect instantaneous player location data, speed, and acceleration across every inch of the field. The NFL's Next Gen Stats have revolutionized football insights and analytics, largely concentrating on offensive metrics. This joint venture now employs machine learning to craft a new defensive measure assessing tackle probability.**\n\n**Information was gathered from tracking data pertaining to the ball and defensive players during each play. Game event details helped identify the defensive team, while tackle data served as the primary outcome variable in a model aimed at predicting a defender's likelihood of executing a tackle.**","metadata":{}},{"cell_type":"markdown","source":"**Python was the tool of choice for data preparation and model construction. Various factors—such as player speed, acceleration, distance from the player to the football, player orientation concerning the football, and player direction in relation to the football—were utilized as features for prediction. Standard scaling was applied to render these features uniform in scale. The output was a binary classification indicating whether the defender successfully tackled or failed to do so. The data was randomly divided into training (80%) and testing (20%) sets. Further splitting of the training data into ten non-overlapping segments (90% training and 10% validation)ensuring optimized model parameters and guarding against overfitting.**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Three distinct machine learning models Gradient Boosting, xgboost, and Custom Deep learning model—were constructed and evaluated to determine the most effective one. The Custom Deep learning model demonstrated superior performance and was thus chosen**.","metadata":{}},{"cell_type":"markdown","source":"**Python script utilizing various libraries and modules for machine learning tasks**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:45:55.757345Z","iopub.execute_input":"2024-01-08T14:45:55.758174Z","iopub.status.idle":"2024-01-08T14:45:55.763212Z","shell.execute_reply.started":"2024-01-08T14:45:55.758142Z","shell.execute_reply":"2024-01-08T14:45:55.762307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data loading process for the NFL Big Data Bowl 2024 competition. It utilizes Pandas, a powerful data manipulation library in Python, to read and manipulate data stored in CSV files**","metadata":{}},{"cell_type":"code","source":"\n\n# Load the datasets from the provided file paths\nplayers = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2024/players.csv')\ntackles = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2024/tackles.csv')\n\n# Load tracking data from all available tracking_week files\ntracking_data = pd.concat([pd.read_csv(f'/kaggle/input/nfl-big-data-bowl-2024/tracking_week_{week}.csv') for week in range(1, 10)])\n\n# Load other relevant datasets if needed (e.g., games, plays, etc.)\ngames = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2024/games.csv')\nplays = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2024/plays.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:45:56.222964Z","iopub.execute_input":"2024-01-08T14:45:56.223694Z","iopub.status.idle":"2024-01-08T14:46:25.470871Z","shell.execute_reply.started":"2024-01-08T14:45:56.223662Z","shell.execute_reply":"2024-01-08T14:46:25.469995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The provided code uses the info() method in Pandas to get concise information about the dataframes players, tackles, tracking_data, games, and plays. This method provides a summary of the dataframe, including the number of entries (rows), the column names, the data type of each column, and the count of non-null values for each column.**","metadata":{}},{"cell_type":"code","source":"\nplayers.info()\ntackles.info()\ntracking_data.info()\ngames.info()\nplays.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.47244Z","iopub.execute_input":"2024-01-08T14:46:25.472726Z","iopub.status.idle":"2024-01-08T14:46:25.51464Z","shell.execute_reply.started":"2024-01-08T14:46:25.472702Z","shell.execute_reply":"2024-01-08T14:46:25.513732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided code snippet includes a function named standardize_date() and subsequent operations applied to the 'players' DataFrame. Here's a breakdown of the code:\n\nstandardize_date() Function:\nPurpose: This function aims to standardize the date format in the 'birthDate' column of the 'players' DataFrame.\nMethod:\nIt attempts to parse the date string using two different date formats ('%y-%d-%m' and '%Y-%d-%m') with pd.to_datetime() and handles potential ValueError exceptions.\nIf the date parsing fails for both formats, it returns the original date string.\nChecks if the parsed date is missing (NaN) and converts it to NaT (Not a Time) if needed.\nFinally, it standardizes the parsed date to the format '%Y-%j-%m', representing year, day of the year, and month.","metadata":{}},{"cell_type":"code","source":"# This function standardizes the date format in the 'birthDate' column of the players_df DataFrame.\n# The code was adapted from a script by Ayush Khaire (https://www.kaggle.com/code/ayushkhaire/players-data-analysis-of-nfl)\ndef standardize_date(date_str):\n    try:\n        parsed_date = pd.to_datetime(date_str, format='%y-%d-%m', errors='raise')\n    except ValueError:\n        try:\n            parsed_date = pd.to_datetime(date_str, format='%Y-%d-%m', errors='raise')\n        except ValueError:\n            return date_str\n    if pd.isna(parsed_date):\n        return pd.NaT\n    standardized_date = parsed_date.strftime('%Y-%j-%m')\n    return standardized_date\nplayers['birthDate'] = players['birthDate'].apply(standardize_date)\nplayers['birthDate'] = pd.to_datetime(players['birthDate'], format='%Y-%j-%m', errors='coerce')\n# Calculate the median birth date for each position\nmedian_dates = players.groupby('position')['birthDate'].median()\nmedian_dates","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.515676Z","iopub.execute_input":"2024-01-08T14:46:25.515974Z","iopub.status.idle":"2024-01-08T14:46:25.69453Z","shell.execute_reply.started":"2024-01-08T14:46:25.515942Z","shell.execute_reply":"2024-01-08T14:46:25.693627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a new DataFrame 'df' based on the 'players' DataFrame, likely to perform operations without NaN values.\nComputing the median birth date for each position after removing rows with missing 'birthDate' values.","metadata":{}},{"cell_type":"code","source":"df = players\ndf = df.dropna()\nmedian_dates = df.groupby('position')['birthDate'].median()\nmedian_dates","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.697364Z","iopub.execute_input":"2024-01-08T14:46:25.697761Z","iopub.status.idle":"2024-01-08T14:46:25.708214Z","shell.execute_reply.started":"2024-01-08T14:46:25.697723Z","shell.execute_reply":"2024-01-08T14:46:25.707227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided code snippet involves a function fill_birth_dates() applied to the 'players' DataFrame for handling missing values in the 'birthDate' column","metadata":{}},{"cell_type":"code","source":"def fill_birth_dates(row):\n    if pd.isna(row['birthDate']):\n        if row['position'] in median_dates.index:\n            return median_dates[row['position']]\n    else:\n        return row['birthDate']\n\n# Apply the function to the 'birthDate' column\nplayers['birthDate'] = players.apply(fill_birth_dates, axis=1)\nplayers.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.709299Z","iopub.execute_input":"2024-01-08T14:46:25.709577Z","iopub.status.idle":"2024-01-08T14:46:25.781726Z","shell.execute_reply.started":"2024-01-08T14:46:25.709552Z","shell.execute_reply":"2024-01-08T14:46:25.780804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_index = players[players['birthDate'].isnull()].index\nplayers.loc[nan_index]","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.782797Z","iopub.execute_input":"2024-01-08T14:46:25.783082Z","iopub.status.idle":"2024-01-08T14:46:25.795458Z","shell.execute_reply.started":"2024-01-08T14:46:25.783058Z","shell.execute_reply":"2024-01-08T14:46:25.794459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the datetime module\nimport datetime\n\n# Create a datetime object for his birth date\nbirth_date = datetime.datetime(1998, 7, 26)\n\n# Find the index of Isaiah Simmons in the DataFrame\nindex = players[players['displayName'] == 'Isaiah Simmons'].index\n\n# Update his birth date in the DataFrame\nplayers.loc[index, 'birthDate'] = birth_date","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.79713Z","iopub.execute_input":"2024-01-08T14:46:25.797475Z","iopub.status.idle":"2024-01-08T14:46:25.805388Z","shell.execute_reply.started":"2024-01-08T14:46:25.797442Z","shell.execute_reply":"2024-01-08T14:46:25.804404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"players['age'] = 2024 - players['birthDate'].dt.year","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.806635Z","iopub.execute_input":"2024-01-08T14:46:25.80722Z","iopub.status.idle":"2024-01-08T14:46:25.814766Z","shell.execute_reply.started":"2024-01-08T14:46:25.807186Z","shell.execute_reply":"2024-01-08T14:46:25.813835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is designed to convert the height from an imperial format (feet and inches) to meters.","metadata":{}},{"cell_type":"code","source":"def convert_height_to_meters(height):\n    # Split the height into feet and inches\n    feet, inches = map(int, height.split('-'))\n    # Convert height to inches\n    total_inches = feet * 12 + inches\n    # Convert inches to cm (1 inch = 2.54 cm)\n    height_cm = total_inches * 2.54\n    # Convert cm to meters\n    height_m = height_cm / 100\n    return height_m\n\nplayers['height_m'] = players['height'].apply(convert_height_to_meters)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:46:25.81604Z","iopub.execute_input":"2024-01-08T14:46:25.816374Z","iopub.status.idle":"2024-01-08T14:46:25.826104Z","shell.execute_reply.started":"2024-01-08T14:46:25.816347Z","shell.execute_reply":"2024-01-08T14:46:25.825229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"players['weight_kg'] = players['weight'] * 0.45359237","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:47:38.466613Z","iopub.execute_input":"2024-01-08T14:47:38.467482Z","iopub.status.idle":"2024-01-08T14:47:38.473074Z","shell.execute_reply.started":"2024-01-08T14:47:38.467447Z","shell.execute_reply":"2024-01-08T14:47:38.472124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"players['bmi'] = players['weight_kg'] / players['height_m']**2\nplayers = players.drop(['height', 'weight', 'height_m', 'weight_kg', 'birthDate'], axis=1)\n# Merge tackles data with player data\ncombined_df_1 = pd.merge(tackles, players, on = 'nflId')\n\n# Then merge with play data\ncombined_df_2 = pd.merge(combined_df_1, plays, on = ['gameId', 'playId'])\n\n# Further merge with games data\ndata = pd.merge(combined_df_2, games, on = 'gameId')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:48:12.427117Z","iopub.execute_input":"2024-01-08T14:48:12.428258Z","iopub.status.idle":"2024-01-08T14:48:12.471451Z","shell.execute_reply.started":"2024-01-08T14:48:12.428215Z","shell.execute_reply":"2024-01-08T14:48:12.470603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Selecting relevant features for analysis**","metadata":{}},{"cell_type":"code","source":"# Selecting relevant features for analysis\nfeatures = ['tackle', 'assist', 'forcedFumble', 'pff_missedTackle', 'position', 'age', 'bmi', 'passResult',\n           'passLength', 'offenseFormation', 'defendersInTheBox','passProbability', 'preSnapHomeTeamWinProbability',\n            'homeFinalScore', 'visitorFinalScore']\neda_df = data[features]","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:48:30.599226Z","iopub.execute_input":"2024-01-08T14:48:30.600135Z","iopub.status.idle":"2024-01-08T14:48:30.608795Z","shell.execute_reply.started":"2024-01-08T14:48:30.600102Z","shell.execute_reply":"2024-01-08T14:48:30.607757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data = data[features]\ncat_columns = sub_data.select_dtypes(['object']).columns\n\n\nsub_data.loc[:, cat_columns] = sub_data.loc[:, cat_columns].apply(lambda x: pd.factorize(x)[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:49:01.885439Z","iopub.execute_input":"2024-01-08T14:49:01.88582Z","iopub.status.idle":"2024-01-08T14:49:01.904572Z","shell.execute_reply.started":"2024-01-08T14:49:01.88579Z","shell.execute_reply":"2024-01-08T14:49:01.903809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_data.loc[:, :].fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:49:15.211914Z","iopub.execute_input":"2024-01-08T14:49:15.2126Z","iopub.status.idle":"2024-01-08T14:49:15.223999Z","shell.execute_reply.started":"2024-01-08T14:49:15.212568Z","shell.execute_reply":"2024-01-08T14:49:15.223046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = sub_data.drop('tackle', axis=1)  # Features\ny = sub_data['tackle']  # Target variable\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:20.624144Z","iopub.execute_input":"2024-01-08T14:50:20.624518Z","iopub.status.idle":"2024-01-08T14:50:20.631576Z","shell.execute_reply.started":"2024-01-08T14:50:20.624489Z","shell.execute_reply":"2024-01-08T14:50:20.63038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\ncorr_matrix = X.corr()\nsn.heatmap(corr_matrix, annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T15:14:24.326043Z","iopub.execute_input":"2024-01-08T15:14:24.326474Z","iopub.status.idle":"2024-01-08T15:14:25.084573Z","shell.execute_reply.started":"2024-01-08T15:14:24.326445Z","shell.execute_reply":"2024-01-08T15:14:25.083637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(X)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T15:20:33.933801Z","iopub.execute_input":"2024-01-08T15:20:33.93418Z","iopub.status.idle":"2024-01-08T15:21:55.226686Z","shell.execute_reply.started":"2024-01-08T15:20:33.93415Z","shell.execute_reply":"2024-01-08T15:21:55.224969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize the features**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nimport numpy as np\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train_minmax = min_max_scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:22.112493Z","iopub.execute_input":"2024-01-08T14:50:22.112887Z","iopub.status.idle":"2024-01-08T14:50:22.182018Z","shell.execute_reply.started":"2024-01-08T14:50:22.11283Z","shell.execute_reply":"2024-01-08T14:50:22.181001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X_train_minmax, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:25.042389Z","iopub.execute_input":"2024-01-08T14:50:25.042749Z","iopub.status.idle":"2024-01-08T14:50:25.050141Z","shell.execute_reply.started":"2024-01-08T14:50:25.04272Z","shell.execute_reply":"2024-01-08T14:50:25.049251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Gradient Boosting **","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:46.689544Z","iopub.execute_input":"2024-01-08T14:50:46.690441Z","iopub.status.idle":"2024-01-08T14:50:46.694876Z","shell.execute_reply.started":"2024-01-08T14:50:46.690396Z","shell.execute_reply":"2024-01-08T14:50:46.693914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\nclf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:48.35419Z","iopub.execute_input":"2024-01-08T14:50:48.354914Z","iopub.status.idle":"2024-01-08T14:50:49.212354Z","shell.execute_reply.started":"2024-01-08T14:50:48.354881Z","shell.execute_reply":"2024-01-08T14:50:49.211351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_clf=clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:50:55.660696Z","iopub.execute_input":"2024-01-08T14:50:55.661081Z","iopub.status.idle":"2024-01-08T14:50:55.670091Z","shell.execute_reply.started":"2024-01-08T14:50:55.661051Z","shell.execute_reply":"2024-01-08T14:50:55.66905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nreport = classification_report(y_test, prediction_clf)\n\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:51:53.18273Z","iopub.execute_input":"2024-01-08T14:51:53.183143Z","iopub.status.idle":"2024-01-08T14:51:53.203512Z","shell.execute_reply.started":"2024-01-08T14:51:53.183108Z","shell.execute_reply":"2024-01-08T14:51:53.2025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **xgboost**\n","metadata":{}},{"cell_type":"code","source":"\nimport xgboost as xgb\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:51:58.528399Z","iopub.execute_input":"2024-01-08T14:51:58.528756Z","iopub.status.idle":"2024-01-08T14:51:58.533056Z","shell.execute_reply.started":"2024-01-08T14:51:58.528728Z","shell.execute_reply":"2024-01-08T14:51:58.531926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n\nxgb_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:51:59.950474Z","iopub.execute_input":"2024-01-08T14:51:59.951352Z","iopub.status.idle":"2024-01-08T14:52:00.119704Z","shell.execute_reply.started":"2024-01-08T14:51:59.951317Z","shell.execute_reply":"2024-01-08T14:52:00.118938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_XGB=xgb_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:52:03.913087Z","iopub.execute_input":"2024-01-08T14:52:03.913756Z","iopub.status.idle":"2024-01-08T14:52:03.922069Z","shell.execute_reply.started":"2024-01-08T14:52:03.913721Z","shell.execute_reply":"2024-01-08T14:52:03.921105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming prediction_XGB contains probabilities or continuous values\nthreshold = 0.5  # Adjust this threshold based on your problem\n\n# Convert probabilities to class labels\nbinary_predictions = np.where(prediction_XGB >= threshold, 1, 0)\n\n# Now use classification_report with y_test and binary_predictions\nreport = classification_report(y_test, binary_predictions)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:52:10.97496Z","iopub.execute_input":"2024-01-08T14:52:10.97707Z","iopub.status.idle":"2024-01-08T14:52:11.000181Z","shell.execute_reply.started":"2024-01-08T14:52:10.977027Z","shell.execute_reply":"2024-01-08T14:52:10.999101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep learning model ** ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the model architecture\nmodel = Sequential()\n\n# Add a dense layer (fully connected layer)\n# Modify input_dim according to your feature dimensions\nmodel.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n# Add more layers if needed\nmodel.add(Dense(64, activation='relu'))\n\n# Output layer - adjust units based on your classification task\n# For binary classification, use 1 unit with sigmoid activation\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:52:22.374414Z","iopub.execute_input":"2024-01-08T14:52:22.374829Z","iopub.status.idle":"2024-01-08T14:52:25.634552Z","shell.execute_reply.started":"2024-01-08T14:52:22.374797Z","shell.execute_reply":"2024-01-08T14:52:25.633749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation loss\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:53:21.768254Z","iopub.execute_input":"2024-01-08T14:53:21.768627Z","iopub.status.idle":"2024-01-08T14:53:22.371177Z","shell.execute_reply.started":"2024-01-08T14:53:21.768596Z","shell.execute_reply":"2024-01-08T14:53:22.370258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Assuming 'model' has been trained and 'X_test' is available\n\n# Predict classes using the trained model\ny_pred = model.predict(X_test)\n# Convert probabilities to class labels\ny_pred = (y_pred > 0.5).astype(int)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted Negative', 'Predicted Positive'],\n            yticklabels=['Actual Negative', 'Actual Positive'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:55:30.739989Z","iopub.execute_input":"2024-01-08T14:55:30.740358Z","iopub.status.idle":"2024-01-08T14:55:31.174054Z","shell.execute_reply.started":"2024-01-08T14:55:30.740327Z","shell.execute_reply":"2024-01-08T14:55:31.172687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# Get predicted probabilities for the positive class\ny_pred_prob = model.predict(X_test)\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc = roc_auc_score(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:55:04.653154Z","iopub.execute_input":"2024-01-08T14:55:04.653891Z","iopub.status.idle":"2024-01-08T14:55:05.112373Z","shell.execute_reply.started":"2024-01-08T14:55:04.653831Z","shell.execute_reply":"2024-01-08T14:55:05.111405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-01-08T15:22:47.308363Z","iopub.execute_input":"2024-01-08T15:22:47.308751Z","iopub.status.idle":"2024-01-08T15:22:47.313259Z","shell.execute_reply.started":"2024-01-08T15:22:47.308719Z","shell.execute_reply":"2024-01-08T15:22:47.312302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve,average_precision_score\nprecision,recall,_ = precision_recall_curve(y_test, y_pred_prob)\naverage_precision = average_precision_score(y_test, y_pred_prob)\nplt.figure()\nplt.plot(recall, precision,color=\"blue\",lw=2, label='Precision-Recall Curve (AP=%0.2f)'%average_precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='lower left')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T15:29:52.31682Z","iopub.execute_input":"2024-01-08T15:29:52.317804Z","iopub.status.idle":"2024-01-08T15:29:52.599813Z","shell.execute_reply.started":"2024-01-08T15:29:52.317764Z","shell.execute_reply":"2024-01-08T15:29:52.598903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'y_test' and 'y_pred' are 2-dimensional arrays with shape (3486, 1)\ny_test_1d = y_test  # Reshape to 1-dimensional array\ny_pred_1d = y_pred.reshape(-1)  # Reshape to 1-dimensional array\n\n# Create DataFrame with reshaped arrays\nsubmission_data = pd.DataFrame({'Actual': y_test_1d, 'Predicted': y_pred_1d})\nsubmission_data.to_csv('submission.csv', mode=\"a\",index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T16:29:05.50048Z","iopub.execute_input":"2024-01-08T16:29:05.501359Z","iopub.status.idle":"2024-01-08T16:29:05.515524Z","shell.execute_reply.started":"2024-01-08T16:29:05.501326Z","shell.execute_reply":"2024-01-08T16:29:05.514591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}